{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6455dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "Collecting nltk>=3.8\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: joblib in c:\\users\\master\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (1.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\master\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (4.66.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\master\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (2023.8.8)\n",
      "Requirement already satisfied: click in c:\\users\\master\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (7.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\master\\anaconda3\\lib\\site-packages (from tqdm->nltk>=3.8->textblob) (0.4.4)\n",
      "Installing collected packages: nltk, textblob\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.6.6\n",
      "    Uninstalling nltk-3.6.6:\n",
      "      Successfully uninstalled nltk-3.6.6\n",
      "Successfully installed nltk-3.8.1 textblob-0.18.0.post0\n",
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\master\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\master\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\master\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\master\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\master\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\master\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n",
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37109c2",
   "metadata": {},
   "source": [
    "#### 딕셔너리기반"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f5ac79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\master\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\master\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# nltk 리소스 다운로드\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f1c732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 간단한 텍스트 데이터 세트\n",
    "reviews = [\n",
    "    \"이 호텔의 위치는 정말 좋았지만, 서비스는 별로였어요.\",\n",
    "    \"음식은 맛있었지만, 분위기가 조금 시끄러웠어요.\",\n",
    "    \"직원들은 매우 친절했고, 방도 깨끗했어요.\",\n",
    "]\n",
    "\n",
    "# 속성 정의\n",
    "attributes = [\"위치\", \"서비스\", \"음식\", \"분위기\", \"직원\", \"방\"]\n",
    "\n",
    "\n",
    "# reviews = [\n",
    "#     \"The location of the hotel was great, but the service was poor.\",\n",
    "#     \"The food was delicious, but the atmosphere was a bit too noisy.\",\n",
    "#     \"The staff were very friendly, and the room was clean.\",\n",
    "# ]\n",
    "\n",
    "# # 속성 정의\n",
    "# attributes = [\"location\", \"service\", \"food\", \"atmosphere\", \"staff\", \"room\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9751fea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 속성별 감성 점수 계산\n",
    "def attribute_sentiment_analysis(reviews, attributes):\n",
    "    attribute_sentiments = {attribute: [] for attribute in attributes}\n",
    "\n",
    "    for review in reviews:\n",
    "        blob = TextBlob(review)\n",
    "        for attribute in attributes:\n",
    "            if attribute in review:\n",
    "                # 속성과 관련된 문장 추출\n",
    "                sentences = [sentence for sentence in blob.sentences if attribute in sentence.string]\n",
    "                sentiment_scores = [sentence.sentiment.polarity for sentence in sentences]\n",
    "                if sentiment_scores:\n",
    "                    average_sentiment = sum(sentiment_scores) / len(sentiment_scores)\n",
    "                    attribute_sentiments[attribute].append(average_sentiment)\n",
    "\n",
    "    # 속성별 평균 감성 점수 계산\n",
    "    attribute_average_sentiments = {}\n",
    "    for attribute, scores in attribute_sentiments.items():\n",
    "        if scores:\n",
    "            attribute_average_sentiments[attribute] = sum(scores) / len(scores)\n",
    "        else:\n",
    "            attribute_average_sentiments[attribute] = None\n",
    "\n",
    "    return attribute_average_sentiments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3341e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_sentiments = {attribute: [] for attribute in attributes}\n",
    "\n",
    "for review in reviews:\n",
    "    blob = TextBlob(review)\n",
    "    for attribute in attributes:\n",
    "        if attribute in review:\n",
    "            # 속성과 관련된 문장 추출\n",
    "            sentences = [sentence for sentence in blob.sentences if attribute in sentence.string]\n",
    "            sentiment_scores = [sentence.sentiment.polarity for sentence in sentences]\n",
    "            if sentiment_scores:\n",
    "                average_sentiment = sum(sentiment_scores) / len(sentiment_scores)\n",
    "                attribute_sentiments[attribute].append(average_sentiment)\n",
    "\n",
    "# 속성별 평균 감성 점수 계산\n",
    "attribute_average_sentiments = {}\n",
    "for attribute, scores in attribute_sentiments.items():\n",
    "    if scores:\n",
    "        attribute_average_sentiments[attribute] = sum(scores) / len(scores)\n",
    "    else:\n",
    "        attribute_average_sentiments[attribute] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66fba308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sentence.sentiment.polarity for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cdf5811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'위치': 0.0, '서비스': 0.0, '음식': 0.0, '분위기': 0.0, '직원': 0.0, '방': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# 감성분석 실행\n",
    "result = attribute_sentiment_analysis(reviews, attributes)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d811d7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.0, subjectivity=0.0)\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"I love TextBlob. It's amazing!\"\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# PatternAnalyzer 사용\n",
    "print(blob.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1507a681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"The food was delicious but the service was terrible.\" has a sentiment score of: 0.0\n",
      "Attribute: food\n",
      "Sentiment-related word: delicious\n",
      "Attribute: service\n",
      "Sentiment-related word: terrible\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# 예문\n",
    "sentence = \"The food was delicious but the service was terrible.\"\n",
    "\n",
    "# TextBlob을 사용하여 간단한 감성 분석 수행\n",
    "blob = TextBlob(sentence)\n",
    "sentences = blob.sentences\n",
    "\n",
    "# NLTK를 사용하여 품사 태깅과 단어 토큰화 수행\n",
    "tokens = word_tokenize(sentence)\n",
    "tagged = pos_tag(tokens)\n",
    "\n",
    "# 감성 점수가 높은 문장(긍정적인)과 낮은 문장(부정적인) 찾기\n",
    "for sentence in sentences:\n",
    "    sentiment = sentence.sentiment.polarity\n",
    "    print(f\"Sentence: \\\"{sentence}\\\" has a sentiment score of: {sentiment}\")\n",
    "\n",
    "    # 감성어와 그에 연관된 명사(속성어) 추출\n",
    "    for word, tag in tagged:\n",
    "        if \"NN\" in tag:  # 명사를 찾음\n",
    "            print(f\"Attribute: {word}\")\n",
    "        elif \"JJ\" in tag:  # 형용사(감성어 가능성 높음)를 찾음\n",
    "            print(f\"Sentiment-related word: {word}\")\n",
    "\n",
    "# 이 코드는 문장에서 감성 점수를 계산하고, 각 단어의 품사를 기반으로 감성어와 속성어를 추출합니다.\n",
    "# 하지만, 이 방법은 매우 기초적이며 실제 응용에서는 더 정교한 방법이 필요할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c2b5c6",
   "metadata": {},
   "source": [
    "----\n",
    "#### 문맥기반으로 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed18a4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81e81500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"The food was delicious but the service was terrible.\", Sentiment: [{'label': 'NEGATIVE', 'score': 0.8429113030433655}]\n",
      "Sentence: \"The room was clean, but the location was not great.\", Sentiment: [{'label': 'NEGATIVE', 'score': 0.9987403750419617}]\n",
      "Sentence: \"The staff were friendly, but the prices were too high.\", Sentiment: [{'label': 'NEGATIVE', 'score': 0.9970555305480957}]\n"
     ]
    }
   ],
   "source": [
    "# 감성 분석 파이프라인 초기화\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# 분석할 문장\n",
    "sentences = [\n",
    "    \"The food was delicious but the service was terrible.\",\n",
    "    \"The room was clean, but the location was not great.\",\n",
    "    \"The staff were friendly, but the prices were too high.\"\n",
    "]\n",
    "\n",
    "# 각 문장에 대한 감성 분석 수행\n",
    "for sentence in sentences:\n",
    "    result = sentiment_pipeline(sentence)\n",
    "    print(f\"Sentence: \\\"{sentence}\\\", Sentiment: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a89379",
   "metadata": {},
   "source": [
    "의존성 구문 분석(Dependency Parsing)과 의미 역할 라벨링(Semantic Role Labeling, SRL)은 자연어 처리(NLP) 분야에서 중요한 개념입니다. 이 두 기술은 문장 내에서 단어 간의 관계와 구조를 이해하는 데 사용됩니다. 그러나 각각의 목적과 방식에는 차이가 있습니다.\n",
    "\n",
    "의존성 구문 분석 (Dependency Parsing)\n",
    "의존성 구문 분석은 문장 내에서 단어들 사이의 의존 관계를 분석하는 과정입니다. 이 방법은 문장의 구조를 이해하는 데 중점을 둡니다. 각 단어는 다른 단어에 대해 의존성을 가지며, 이러한 의존성을 통해 문장의 구조적인 특성을 파악할 수 있습니다.\n",
    "\n",
    "목적: 문장 내에서 단어들 사이의 '구문적' 관계를 파악하는 것입니다. 예를 들어, 어떤 단어가 동사에 대한 주어인지, 객체인지 등을 식별합니다.\n",
    "예시: \"The cat sat on the mat.\"에서 \"cat\"은 \"sat\"에 대한 주어(subject)이고, \"mat\"은 \"on\"의 객체(object)입니다. 의존성 구문 분석은 이러한 관계를 트리 형태로 나타냅니다.\n",
    "의미 역할 라벨링 (Semantic Role Labeling, SRL)\n",
    "의미 역할 라벨링은 문장 내에서 각 단어나 구가 수행하는 '의미적 역할'을 식별하는 과정입니다. 이 방법은 문장의 의미를 이해하는 데 중점을 둡니다. SRL은 동사와 관련된 주체, 객체, 시간, 장소 등의 의미적 역할을 분류합니다.\n",
    "\n",
    "목적: 문장 내에서 주요 동사의 인자들(argument)과 그 인자들이 수행하는 역할을 파악하는 것입니다. 예를 들어, 누가(주체) 무엇을(객체) 언제(시간) 어디서(장소) 등의 질문에 답할 수 있게 합니다.\n",
    "예시: \"John gave Mary a book on Monday.\"에서 John은 \"gave\"의 주체(Agent), Mary는 수혜자(Recipient), \"a book\"은 주어진 객체(Theme), \"on Monday\"는 시간(Time)입니다. SRL은 이러한 역할을 식별하여 라벨을 부착합니다.\n",
    "의존성 구문 분석과 의미 역할 라벨링은 서로 보완적인 정보를 제공합니다. 의존성 구문 분석은 문장의 구조적인 측면을, SRL은 의미적인 측면을 강조합니다. 함께 사용될 때, 이 두 기술은 자연어 이해(Natural Language Understanding, NLU) 작업에서 강력한 도구가 될 수 있습니다. 예를 들어, 질의 응답(Question Answering), 정보 추출(Information Extraction), 기계 번역(Machine Translation) 등 다양한 NLP 작업에서 중요한 역할을 합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
